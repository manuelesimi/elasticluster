Notes on deploying ceph *manually*
==================================

Most of the docs on setting up ceph either use `ceph-deploy` or
`mkcephfs`. However, these tools are not suitable for an automatic
deploying system like Ansible, CFEngie or Puppet, and they hide a lot
of the complexity behind the process. However, hiding the complexity
of the instlalation is good only when things goes smoothly, while when
you are trying to understand why it didn't work you find yourself
clueless on how to debug the problems.

Here I'm writing a few notes on a *manual* ceph deployment, stuff I've
found out trying to create the ansible playbooks.

The stuff I am not sure about are marked as **NOT SURE:**. The other
stuff I am pretty confident that, at the time of writing 
(28.05.2013, ceph version 0.61.2), is working as I am saying.

**NOT SURE:** however, I sometimes make mistakes too :)

Background
----------

To setup ceph you need:

1) one or more monitor nodes
2) one or more osd nodes
3) *optionally*, one or more mds nodes

Unless you are explicitly disabling the ``cephx`` authentication
(http://ceph.com/docs/next/rados/operations/authentication/#disabling-cephx),
one of the things you can find tricky is setting up the monitor node,
especially the keyring.

The easiest way to create a working keyring is to create it on one of
the node and then copying it in the correct locations of the various
nodes. This is what I've done, and it work. I am pretty sure that I'm
copying *more* information than the necessary though...

Since a ceph cluster needs at least one monitor node, I am creating
the keyring in the monitor node and then copying the file to the other
nodes.

Setting up the monitor node
---------------------------

Setting up a monitor (in our case) consists of the following steps:

1) create a keyring. The keyring will have keys and correct
   capabilities for the following services/clients:

   ``mon.`` 
       this is, as far as I understood, the key that will be
       shared among all the monitor nodes. **NOT SURE:** I don't know
       if you need one key for all of them or you may just have one
       key for each one of them and share all of them. However, I have
       both...

   ``client.admin``
       this will be used by the administrative commands,
       and can be used to mount the ``cephfs`` filesystem.

   ``osd.<name>``
       one key for each osd node.

   ``mds.<name>``
       one key for each mds node.

2) Bootstrapping the monitor
   (cfr. also http://ceph.com/docs/master/dev/mon-bootstrap/)

Creating the keyring
++++++++++++++++++++

The first step is quite easy, I use ``/etc/ceph/ceph.mon.keyring`` as
temporary keyring file, and then I will copy this file in the
locations where the various daemons are expecting to find it. I run
all these commands in the first monitor node.

First of all you have to create a shared key with the special name
``mon.`` (it's not a typo, it's exactly: `m`, `o`, `n` and `.` (dot))

This is the command::

  ceph-authtool /etc/ceph/ceph.mon.keyring --create-keyring \
      --gen-key -n mon. \
      --cap mon 'allow *'

Since the file is missing, we use the ``-create-keyring`` option, and
we also have to specify the capabilities with ``--cap mon 'allow *'``.

Then we create a key for the ``client.admin`` user, which is used by
the administrative commands and can be used to mount the cephfs
filesystem::

    ceph-authtool /etc/ceph/ceph.mon.keyring --gen-key \
        -n client.admin \
        --cap mon 'allow *' \
        --cap osd 'allow *' \
        --cap mds 'allow *'

In this case the user will need to access all the various services,
*mon*, *osd* and *mds*.

Then, we need one mds key for each metadata server. They all need to
access also the monitor and the OSDs::

    for mdsname in $mdslist
    do
        ceph-authtool /etc/ceph/ceph.mon.keyring --gen-key \
            -n mds.$mdsname \
            --cap mds 'allow *' \
            --cap osd 'allow *' \
            --cap mon 'allow rwx'
    done

Do the same for the OSDs, but in this case the ``--cap`` options are
different::

    for osdname in $osdlist
    do
        ceph-authtool /etc/ceph/ceph.mon.keyring --gen-key \
            -n osd.$osdname \
            --cap osd 'allow *' \
            --cap mon 'allow rwx'
    done

This file **must** be present in each ``mon data`` directory on each
mon node. Usually, this is ``/var/lib/ceph/mon/ceph-<name>``, but *in
principle* you should be able to change it.

Bootstrapping the monitor
+++++++++++++++++++++++++

This can be done in multiple ways. I used a *monmap* file, which
contains information about the monitors belonging to the cluster (I
will not enter in the discussion of quorum nodes, peers etc, please
read the ceph documentation for that.)

If you use the *monmap* way, you need to:

1) create the *monmap* file
2) run ``ceph-mon --mkfs --monmap <file> ...``

There are a few issues with the *monmap* file, which is generated by
the ``monmaptool``:

* It does not take automatically information from the configuration
  file ``/etc/ceph/ceph.conf``, you have to pass the ``-c`` option.

* If you don't do it, and you use the ``--set-initial-members`` and
  the ``-m`` option, it will use default values which are probably not
  good for you.


Before using the monmap file, I **strongly** suggest you to inspect
its content with the ``monmaptool --print <filename>`` command. For
instance, the following command::

    root@ceph-mon001:~# monmaptool --create --generate -m ceph-mon001 /tmp/monmap.worng
    monmaptool: monmap file /tmp/monmap.worng
    monmaptool: generated fsid 5218f76d-ca8d-4f8d-8599-8802c327e7ae
    monmaptool: writing epoch 0 to /tmp/monmap.worng (1 monitors)

Will create a wrong file. To inspect its content run::

    root@ceph-mon001:~# monmaptool --print /tmp/monmap.worng 
    monmaptool: monmap file /tmp/monmap.worng
    epoch 0
    fsid 5218f76d-ca8d-4f8d-8599-8802c327e7ae
    last_changed 2013-05-28 21:12:58.052174
    created 2013-05-28 21:12:58.052174
    0: 10.10.10.14:6789/0 mon.noname-a

As you can see, the only monitor defined has the correct ip address but
**wrong name**: ``mon.noname-a``.

Also, the ``fsid`` is automatically generated every time you run the
command, which means that if you already defined a ``fsid`` in the
``/etc/ceph/ceph.conf`` configuration file, this *monmap* will **not**
work!

On the other hand, assuming the following snippet from the
``/etc/ceph/ceph.conf``::

    [global]
        auth cluster required = cephx
        auth service required = cephx
        auth client required = cephx

        fsid = 00baac7a-0ad4-4ab7-9d5e-fdaf7d122aee
    [mon.0]
        host = ceph-mon001
        mon addr = 10.10.10.14:6789
        mon data = /var/lib/ceph/mon/ceph-0
    [mon.1]
        host = ceph-mon002
        mon addr = 10.10.10.17:6789
        mon data = /var/lib/ceph/mon/ceph-1
    [mon.2]
        host = ceph-mon003
        mon addr = 10.10.10.20:6789
        mon data = /var/lib/ceph/mon/ceph-2

Running ``monmaptool``::

    root@ceph-mon001:~# monmaptool  --create --generate -c /etc/ceph/ceph.conf /tmp/monmap.right
    monmaptool: monmap file /tmp/monmap.right
    monmaptool: set fsid to 00baac7a-0ad4-4ab7-9d5e-fdaf7d122aee
    monmaptool: writing epoch 0 to /tmp/monmap.right (3 monitors)

Will correctly generate the monmap file::

    root@ceph-mon001:~# monmaptool --print /tmp/monmap.right 
    monmaptool: monmap file /tmp/monmap.right
    epoch 0
    fsid 00baac7a-0ad4-4ab7-9d5e-fdaf7d122aee
    last_changed 2013-05-28 21:20:41.032373
    created 2013-05-28 21:20:41.032373
    0: 10.10.10.14:6789/0 mon.0
    1: 10.10.10.17:6789/0 mon.1
    2: 10.10.10.20:6789/0 mon.2

After creating the *monmap* file you can create the *filesystem* in
the ``mon data`` directory. This command has to be run **on each
monitor node**, and replace ``$monname`` with the correct name (in the
previous configuration, it would be `0`, `1` or `1`)::

    ceph-mon --mkfs -i $monname --monmap /etc/ceph/monmap \
        --keyring /etc/ceph/ceph.mon.keyring

Now you should have a ``store.db`` directory in ``mon data``, and you
should be able to run the mon with ``service ceph start``.

Commands to check the status of the monitor:

``ceph auth list``
    prints the list of keys and their capabilites

``ceph mon dump``
    prints a list of the mon nodes, similar to the output of
    ``monmaptool --print``

``ceph status``
    prints information about the status of the cluster.

If something went wrong, follow the instructions on how to increase
the debugging level at
http://ceph.com/docs/master/rados/troubleshooting/log-and-debug/ and
in case you need to run using strace, all the various ``ceph-mon``,
``ceph-osd`` and ``ceph-mds`` daemon accept a ``-d`` option to run in
foreground and print information on the standard output instaead of
the log file. Unfortunately not all the messages are meaningful...


Setting up the OSD
------------------

**TODO**

1) Copy the key from the monitor node
2) create the filesystem.
   ceph-disk does not create all the needed files! I had to run also
   ceph-osd --mkfs --mkjournal
3) run ``ceph osd create``

ceph-disk list show information on the disk

In principle you don't need to store ``osd data`` on a different disk,
but:

* /etc/init.d/ceph assumes it and tries to mount the filesystem if
  it's not mounted, and fails if no ``devs`` is defined.

* for the same reason, the init script fails if no ``osd mkfs type``
  is defined, because it uses it to know how to mount the device in
  the ``osd data`` directory.

* you need specific features of the filesystem that may not be present
  in the default filesystem.

* I am not sure how ceph deal with the available space if you have a
  promiscuous data directory.

* I think that some other parts of the code is assuming that the data
  directory is on a separate filesystem.

Setting up the MDS
------------------

This is the easiest step. You only have to copy the related key in
``mds data``.

**TODO**

Notes on the configuration file
-------------------------------

A few random remarks:

* I had to **remove** the ``mon initial members`` option from the
  configuration file. Apparently, having this option caused the
  monitors to *never* establish the quorum, even when it was the only
  monitor node.

* I put ``osd addr``, ``mon addr`` and ``mds addr`` for all the
  hosts. I had a few problems not putting them in the configuration
  file, not sure which ones though, and I am still not sure they are
  needed.

* **NOT SURE** I am not sure if you can actually use the same
  directory for all the services, or all the osd services. You
  probably can, but I still feel that some parts of the code is
  looking for ``/var/lib/ceph/osd/ceph-<name>``, for instance for
  OSDs, so I followed the same syntax.

* *in principle*, you don't need to dedicate a volume for the osd data
  dir; however, the init script in ``/etc/init.d/ceph`` will file if
  you don't use one, and I think that other parts of the code is
  assuming the the data directory actually resides on an external
  filesystem. This means that **you need** to define ``devs`` and
  ``osd mkfs type`` in the configuration file.

Mounting CephFS
---------------

Assuming everything went well, assuming you have at least *one*
**mds** node, mounting the ceph filesystem is quite easy.

1) First of all, you need to load the ``ceph`` kernel module::

       module load ceph

2) then, you need to use a key of an usre that has the right
   capabilites. **NOT SURE** which these are, but the ``client.admin``
   user we created at the beginning works. Run the command::

       ceph auth list

   and then look for an output similar to::

       client.admin
        key: AQCQH6VRQEAPBxAAro3n7bvA8oYUKt5CevCdDg==

   The base64-encoded string after ``key: `` is the key.

3) mount the filesystem using the key of the ``client.admin`` user::

       mount -t ceph <mon-ip>:6789:/ /mnt -o name=admin,secret=<key>

   where:

   ``<mon-ip>``
       is the ip or the hostname of one of the monitor hosts

   ``name=admin``
       is the user you want to use. In this case, ``name=admin`` means
       that the monitor will check that the following ``secret``
       corresponds to the key of the ``client.admin`` user.

   ``secret=<key>``
       is the key of the ``client.admin`` user (or ``client.<foo>`` if
       you specified the ``name=<foo>`` option)
